from pathlib import Path

import torch
import torch.nn as nn
from torch import Tensor

from lightning.pytorch import LightningModule

from noposplat.model.encoder import get_encoder, EncoderCfg
from noposplat.model.types import Gaussians

from gspipline.renderer.gaussian_splatting_render import GaussianSplattingRender, GaussianSplattingConfig
from gspipline.pipeline.mixins import LogMaxin

from dataclasses import dataclass, field


@dataclass
class ImageEncoderConfig:
    repo_or_dir: str = 'facebookresearch/dinov2'
    """rep or dir"""
    model: str = "dinov2_vitg14"
    """model"""

@dataclass
class DiscriminatorHeadConfig:
    ...

@dataclass
class LearningConfig:
    """learning rate"""
    base_lr: float = 0.0001
    """base learning rate"""
    encoder_lr_mult: float = 1.0
    """encoder learning rate multiplier"""
    head_lr_mult: float = 1.0
    """head learning rate multiplier"""
    encoder_trainable: bool = False
    """whether to train the encoder"""

@dataclass
class GaussianDiscriminatorPipelineConfig:
    encoder: EncoderCfg = field(default_factory=EncoderCfg)
    """ encoder config """
    encoder_pretrained_ckpt: Path = None
    """ pretrained checkpoint for the encoder """
    renderer: GaussianSplattingConfig = field(default_factory=GaussianSplattingConfig)
    """ renderer config """
    image_encoder: ImageEncoderConfig = field(default_factory=ImageEncoderConfig)
    """ image encoder config """
    discriminator_head: DiscriminatorHeadConfig = field(default_factory=DiscriminatorHeadConfig)
    """ discriminator head config """
    learning: LearningConfig = field(default_factory=LearningConfig)
    """ learning rate config """


class GaussianDiscriminatorPipeline(LightningModule, LogMaxin):
    """ 
    Pipeline for training a discriminator to distinguish between real and fake images generated by the GaussianSplattingRender.
    The discriminator takes the rendered image and the features extracted from the image encoder as input, and outputs a probability of the image being real.
    """
    config: GaussianDiscriminatorPipelineConfig
    
    def __init__(self, config: GaussianDiscriminatorPipelineConfig):
        """
        Args:
            config (GaussianDiscriminatorPipelineConfig): configuration for the pipeline
        """
        super().__init__()
        self.config = config
        self._init_loss()
        self._init_gaussian_encoder()
        self._init_renderer()
        self._init_discriminator()

    def _init_loss(self):
        self.bce_loss = nn.BCEWithLogitsLoss()

    def _init_gaussian_encoder(self):
        self.gaussian_encoder, _ = get_encoder(self.config.encoder)
        state_dict = torch.load(self.config.encoder_pretrained_ckpt)['state_dict']
        state_dict = {k[len("encoder."):] : v for k, v in state_dict.items() if k.startswith("encoder.")}
        self.gaussian_encoder.load_state_dict(state_dict)
        self.gaussian_encoder.eval()
    
    def _init_renderer(self):
        self.renderer = GaussianSplattingRender(self.config.renderer)
        self.renderer.eval()

    def _init_discriminator(self):
        self.image_encoder: nn.Module = torch.hub.load(self.config.image_encoder.repo_or_dir, self.config.image_encoder.model)
        self.image_encoder.eval()

        self.head = nn.Sequential(
            nn.Linear(self._get_image_encoder_features_dim(), 512),
            nn.ReLU(True),
            nn.Linear(512, 1)
        )

    def _get_image_encoder_features_dim(self):
        return self.image_encoder.norm.weight.shape[0]
    
    def _predict_gaussian(self, batch: dict) -> dict:
        """
            Args:
                batch (dict): a batch of data, containing:
                    - "image" (torch.Tensor): refence image, shape (B, v, 3, H, W)
                    - "mask" (torch.Tensor): optional, mask for refence image, shape (B, v, 1, H, W)
                    - "pose" (torch.Tensor): optional, pose for refence image, shape (B, 4, 4), nerfstudio coordinate system
                    - "intrinsics" (torch.Tensor): intrinsics for refence image, shape (B, v, 3, 3), fx, fy, cx, cy in normalized coordinates
            Returns:
                dict: a dictionary containing the predicted gaussian parameters, including:
                    - "means" (torch.Tensor): shape (B, HW, 3)
                    - "covariances" (torch.Tensor): shape (B, HW, 3, 3)
                    - "harmonics" (torch.Tensor): shape (B, HW, k, 3)
                    - "opacities" (torch.Tensor): shape (B, HW, k)
                    - "scales" (torch.Tensor): shape (B, HW)
                    - "rotations" (torch.Tensor): shape (B, HW, 4)
                    - "depth" (torch.Tensor): shape (B, HW)
        """
        visualization_dump = {}
        gaussian: Gaussians = self.gaussian_encoder(
            context = {
                "image": batch["image"],
                "intrinsics": batch["intrinsics"],
            },
            global_step = 0, # TODO: use self.global_step
            visualization_dump = visualization_dump,
        )
        return {
            'means': gaussian.means,
            'covariances': gaussian.covariances,
            'colors': gaussian.harmonics.permute(0, 1, 3, 2),
            'opacities': gaussian.opacities,
            'quats': visualization_dump['rotations'],
            'scales': visualization_dump["scales"],
            'depth': visualization_dump["depth"],
        }

    def _render_gaussian(self, gaussians: dict, batch: dict) -> dict:
        """
            Args:
                gaussians (dict): a dictionary containing the predicted gaussian parameters, including:
                    - "means" (torch.Tensor): shape (B, HW, 3)
                    - "quats" (torch.Tensor): shape (B, HW, 4)
                    - "scales" (torch.Tensor): shape (B, HW)
                    - "colors" (torch.Tensor): shape (B, HW, K, 3)
                batch (dict): a batch of data, containing:
                    - "camtoworlds" (torch.Tensor): shape (B, n, 4, 4)
                    - "Ks" (torch.Tensor): shape (B, n, 3, 3)
                    - "width" (int): image width
                    - "height" (int): image height
            Returns:
                dict: a dictionary containing the rendered image, including:
                    - "images" (torch.Tensor): shape (B, n, 3, H, W)
                    - "mask" (torch.Tensor): shape (B, n, 1, H, W)
                    - "depth" (torch.Tensor): shape (B, n, H, W)
        """
        B = gaussians['means'].shape[0]
        images = []
        masks = []
        for i in range(B):
            render_colors, render_alphas, info = self.renderer(
                    means=gaussians['means'][i],
                    quats=gaussians['quats'][i],
                    scales=gaussians['scales'][i],
                    opacities=gaussians['opacities'][i],
                    colors=gaussians['colors'][i],
                    camtoworlds=batch['camtoworlds'][i],
                    Ks=batch['Ks'][i],
                    width=batch['width'],
                    height=batch['height'],
                    need_active=False,
                )
            images.append(render_colors)
            masks.append(render_alphas)
        return {
            "images": torch.stack(images, dim=0),
            "mask": torch.stack(masks, dim=0),
        }
    
    def forward(self, images: Tensor, masks: Tensor = None) -> dict:
        """
            Args:
                images (torch.Tensor): shape (B, 3, H, W)
                masks (torch.Tensor): optional, shape (B, 1, H, W)
            Returns:
                dict: a dictionary containing the predicted probability of the image being real, and the features extracted from the image encoder.
                    - "pred" (torch.Tensor): shape (B, 1)
                    - "feature" (torch.Tensor): shape (B, D)
        """
        with torch.set_grad_enabled(self.config.learning.encoder_trainable and self.training):
            features: Tensor = self.image_encoder(images, masks)
        pred: Tensor = self.head(features)
        pred = torch.sigmoid(pred)
        return {
            "pred": pred,
            "feature": features,
        }
    
    def _get_loss(self, preds: dict, targets: dict) -> dict:
        """
            Args:
                preds (dict): a dictionary containing the predicted probability of the image being real, and the features extracted from the image encoder.
                    - "pred" (torch.Tensor): shape (B, 1)
                    - "features" (torch.Tensor): shape (B, D)
                targets (dict): a dictionary containing the target information, including:
                    - "target" (torch.Tensor): shape (B, 1)
            Returns:
                dict: a dictionary containing the loss and the output of the model.
                    - "bce_loss" (torch.Tensor): binary cross entropy loss
                    - "loss" (torch.Tensor): total loss
        """
        loss = 0
        bce_loss = self.bce_loss(preds["pred"], targets["target"])
        loss += bce_loss
        return {
            "bce_loss": bce_loss,
            "loss": loss
        }

    def training_step(self, batch: dict, batch_idx: int, dataloader_idx: int = 0):
        """
            Args:
                batch (dict): a batch of data, containing
                    - "image" (torch.Tensor): refence image, shape (B, v, 3, H, W)
                    - "mask" (torch.Tensor): optional, mask for refence image, shape (B, v, 1, H, W)
                    - "pose" (torch.Tensor): optional, pose for refence image, shape (B, 4, 4), nerfstudio coordinate system
                    - "intrinsics" (torch.Tensor): optional, intrinsics for refence image, shape (B, 3, 3), fx, fy, cx, cy in normalized coordinates
                    - "novel_image" (torch.Tensor): optional, novel image, shape (B, n, 3, H, W)
                    - "novel_mask" (torch.Tensor): optional, mask for novel image, shape (B, n, 1, H, W)
                    - "novel_pose" (torch.Tensor): pose for novel image, shape (B, n, 4, 4), nerfstudio coordinate system
                    - "novel_intrinsics" (torch.Tensor): intrinsics for novel image, shape (B, n, 3, 3), fx, fy, cx, cy in normalized coordinates
                Returns:
                    dict: a dictionary containing the loss and the output of the model.
        """
        assert 'image' in batch
        assert 'novel_image' in batch
        assert 'novel_pose' in batch
        assert 'novel_intrinsics' in batch
        
        with torch.no_grad():
            gaussians = self._predict_gaussian(batch)

        batch['viewmats'] = batch['novel_pose']
        batch['camtoworlds'] = batch['novel_pose']
        batch['Ks'] = batch['novel_intrinsics']
        batch['width'] = batch['image'].shape[-1]
        batch['height'] = batch['image'].shape[-2]
        with torch.no_grad():
            renderings = self._render_gaussian(gaussians, batch)

        preds = self(images=torch.cat([batch['image'].unsqueeze(1), renderings['images'].unsqueeze(1)], dim=0))

        losses = self._get_loss(preds, batch=batch)

        self.log_loss(losses)

        return {
            "loss": losses["loss"],
        }

    def configure_optimizers(self) -> torch.optim.Optimizer:
        """
            Returns:
                torch.optim.Optimizer: optimizer for the model
        """
        params = []
        params.append({"params": self.head.parameters(), "lr": self.config.learning.base_lr * self.config.learning.head_lr_mult})
        if self.config.learning.encoder_trainable:
            params.append({"params": self.image_encoder.parameters(), "lr": self.config.learning.base_lr * self.config.learning.encoder_lr_mult})
        return torch.optim.Adam(params)